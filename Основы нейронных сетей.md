## Математический нейрон
Мак-Каллок и Питтс - Идея матемаческих вычислений в нервной деятельности. Они предложили рассмотреть их на самом низком уровне: Входно-выходной (несколько вводов, выполняем с ними вычисление в теле и выводим результат), $S= \sum{x_i *W_c}$ - порог чувствительности нейронов. y=0, если S<theta; y=1, если S≥theta
Графическая интерпретация - функция-ступеньки (как вероятность)
### Логические операции
#### И
x1, x2= 1, theta = 2
#### Или
x1/x2= 1, theta = 1
#### Не
x1 = -1, theta = 0
### Нейронное смещение
b=-theta
$S = \sum{W_i *X_j}$

чёто отвлёк не услыхал, сам ищи :))))
### Персептрон Розенблата
Первый нейронный компьютер. Он умел различать латинские цифры: накладывали фотоматрицу, на которой было изображена цифра. Значение пустые клеток = 0, остальные = 1.
Розенблат использовал итерационный алгоритм корректировки синапсических весов: 
1. Значение случайных чисел присваивалось всем значениям весов и порогу чувствительности
2. Компьютеру предоставляли фотоматрицу с числом
3. Суммирование входных сигналов нейроном
	1. Выходной сигнал - правильный?  - увеличиваем веса активных входов ($W_j (t+1) =W_j (t) + X_j$, t - эпоха; W(t) - значение на предыдущей эпохе. Правило Хебба
	2. Выход - неправильный и результат= 1 - веса уменьшаем (вместо + -). Второе правило Хебба 
4. Переход на пункт 2 или завершение.
$\epsilon=d-y$. Эпсилон= 0 - хорошо, шаг 3.1. Эпсилон!=0 - плохо, шаг 3.2
### Коэффициент скорости обучения
Рекомендуется [0.05, 1.5]
Алгоритм с использующий формулы выше называется дельта-правило.
### Персептрон со скрытым слоем нейронов
(с перекрёстным подключением связей к нейронам)
1. s1=0.5x1 ±0.5x2. s1≥0 -> y1=1;s1<0->y1=0
2. S2=±0.5x1+0.5x2
4. S3=y1+y2
### многослойный персепторн и алгоритм его обучения
Основная цель заключается в ошибки нейронов выходного слоя используются для вычисления ошибок нейронов в скрытых слоях

**@ аз@

Обобщённое дельта-правило 
$W_{ij}(t+1) =W_{ij}(t)+ \delta W_{ij}$
$\delta W_{ij} = ñ \beta y_i$ ,ñ- скорость обучения 
Синапсические веса внешнего слоя корректируются по тому же правилу.
### Методы моделирования
- градиентного спуска. 
	Его  задача поиска минимальной целевой функции. $\epsilon = \epsilon(\omega)->min$, - $\epsilon$ - ошибка персептрона, $\omega$ - вектор синапсических весов. Процесс поиска ошибки представляется как:
	- поиск в некоторой заданной точке поверхности ошибок направления скорейшего спуска - антиградиента;
	- загон делает шаг вниз на r пропорционально крутизне схода и к скорости обучения.
Во всех методах существует проблема выбора параметров скорости обучения, определяющих длину шага вдоль направления оптимизации. $\nu ≤ min(\frac{1}{n_i})$, где $n_i$- количество входов в i слое. 
- эвристические методы не имеют строгого теоретического основания. 
Они отражают личный опыт работ автора нейросети. 
По алгоритму RPROP корректировка весов учитывает только знак соответствующего градиента: 
$$sign(S_{ij}(t)) = \frac{\delta\epsilon(t)}{\delta\omega_{ij}}$$
- неградиентные - просто не требуют градиент. Как правило ошибок нет. 
- эволюционные вычисления - совокупность подходов решения ошибок (оптимизация). В её основе лежит эаолюционная идея. 
#### Базовые элементы эволюции 
#### Модели классификации и её метрики
Для оценки полноты и точности вводится
- матрица нечетности. Она показывает, какие ошибки могут встречаться: TP - true positive, FP - false positive, FN - false negative, TN - true negative. Точность = TP /(TP+FP); полнота= TP/(TP+FN )
- F-мера: взаешенная оценка меры, позволяет управлять точность и полнотой![[IMG_20231114_111549.jpg]]
- ROC AUC - дополнительная независимая оценка качества работы модели. ROC - кривая качества работы классификатора; AUC - площадь под ней. Координаты: TPR (true positive rate), FPR (false positive rate)
TPR = TP/(TP+FP)
FPR = FP/(TN+FP)
Специфичность = 1-FPR
Для него является нормированным критерий Gini: gini=ROCAC
- оценка каппа коэна. ![[IMG_20231114_113334.jpg]]
Взвешенную каппц используют при более трёх классов, поскольку в матрцу весов добавляется и нормируется любое число. Она совмещает в чебе много подоходв к оценке правильности раьоты классификатора. И отлично показывает себя хорошо при большом кве классов, где нудно соотнести близке и дальние промахи 


#### чёто странное
Структура сети подбирается количеством 
1. Входных нейронов (оно должно быть равно размерности входного вектора X)
2. выходных нейронов равно размерности выходного вектора D 
3.  скрытых слоёв - не менее одного, но может корректироваться. 
4.  скрытых нейронов $N =N_w / (N_x+N_y)$
5. А их активационные функции рекомендуется задать сигмоидной функцией, но её вид также может быть изменён. При этом чрезмерное уменьшение количество скрытых слоёв приводит к гиперазмерности (потере обобщающих св-в сети)
#### обучение
Его цель - подбор синапсических весов для соответствия количества векторов X вектору Y, минимально отличный от выходного вектора. 
##### причины недообучения
- недостаточное количество скрытых слоёв и нейронов 
- наличие в выборке конфликтных данных/примеров (удаление)
- попадание в локальный минимум
- наличие в данных выбросов (примеров, не подчиняемых законам предметной области)
- паралич сети - процесс обучения остановлен в результате неуправляемого роста синапсических весов и аргументов активационной функции. берут другую функцию без вертикальных асимптот/нормализация
- слишком большая скорость обучения (при большой скорости теряется устойчивость итерационного процесса )
#### этап проверки/оптимизация и тестирование сети
Цель этапа - понижение ошибок и поиск оптимального значения 
Называется так, поскольку он производится на тестирующем множестве. Сопоставление желаемого выхода с выходным значением: если разница не значительна, то этап пропускается. но если она большая, то необходимо оптимизировать сеть (перейти на предыдущие этапы). 
После оптимизации сети необходимо проверить обобщающее свойства сети с помощью подтверждающего множества 
#### анализ 
Проверка входа и выхода
#### метод опорных векторов svm support vector method 
Это алгоритм машинного обучения, решающий задачу классификации постройкой гиперплоскости для разделения объектов на несколько классов. 
Его цель - найти оптимальную гиперплоскость в N-мерном пространстве, разделяющую классы. 
Он используется как для линейно так и для не линейно разделяемых данных. 
Гиперплоскость - средняя линия максимальной площади, проходящей боками через крайние значения 
##### линейная классификация
Линейный классификатор заключается в разделении признакового пространства гиперплоскостью на два полупространства, в которых прогнозируется одно и два значения целевого класса. 
Сделано без ошибок - линейно разделима 
##### терминология свм
Гиперплоскость - границы принятия решений, используемая для разделения точек данных для разных классов
Опорные векторы - ближайшие точки данных гиперплоскости
Запас - расстояние между опорным вектором и гиперплоскостью
Задача алгоритма свм - максимизировать запас, увеличение производительности классификации
Ядро - математическая функция, используемая для отображения исходных точек входных данных в пространстве объектов более высокой размерности
Функции ядра: 
- линейная
- полиномиальная
- радиально-базисная
- сигмоидальная
Жёсткое поле - гиперплоскость с максимальным полем, которое надлежащим образом разделяет точки данных разных категорий без каких-либо неправильных классификаций
Мягкие границы - данные не являются полностью разделимыми/имеются выбросы
Двойная задача - задача оптимизации, требующая определения местоположения множителей лагранжа. 
##### функции ядра 
###### Перевод точки в пространство большей размерности:
2->3
ū =[x, y], делаем скалярное произведение. 
ū•ū =x^2+y^2 
ū =[x, y, x^2+y^2]


o(u)^T o(v)= sum i, j=1 xixjyiyj
###### трюк с ядром
Он позволяет работать с данными более высокого измерения
###### виды ядер 
- полиномиальное k(xi, xj) = (xi*xj+1)^d
- ядро rbf лапласа exp(-||xi-xj||/sigma)
- радикальная базисная функция Гаусса (rbf) exp(-gamma||xi-xj||^2)
- ядро Гаусса exp(-||xi-xj||^2/2sigma^2)
- сигмовидное tanh(alpha*xi^T * xj*+x), alpha - наклон, c - точка пересечения 
##### подытог
Pros: 
- наиболее быстрый метод нахождения решающих функций
- сводится к решению задачи квадратичного программирования выпуклой области, имеющей единственное решение
- находит разделяющую полосу максимальной ширины
Cons:
- вычислительно затратный, невозможность масштабирования
#### наивный алгоритм байеса
Это вероятностный алгоритм, используемый в машинном обучении для задачи классификации. Основан на т. Байеса о вероятности события. Он предлагает, что признаки независимы друг от друга (поэтому наивный).
т. Байеса: p(a/b) = p(b/a)*p(a)/p(b)
Максимальная апостериорная оценка argmax_y*p(y) prod(i=1, n)
##### типы наивных байесовских классификаторов
- полиномиальнаое - векторы есть частоты, с которыми определённые события генерируются полиномиальным распределением
- Бернулли - характеристики бинарного термина, а не частота его
- Гауссово- признаки подвергаются нормлаьному распределению
Pros:
- быстрый и лёгкий
- хорошая производительность и можно меньше обучающих данных
- хорошо работает с категориальными признаками
Cons:
- наивный
- в тексте может переобучиться, если нет сглаживания Лапласа 
- для всех других функций (кроме категориальных) мало что может