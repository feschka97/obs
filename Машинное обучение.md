Машинное обучение состоит из: 
- ввод выборки
- извлечение признаков
- классификация
- получение результата
### Его составляющие:
- данные
- статистика
- расчёты
- другая информация для обучения
- признаки
- алгоритмы (для выбора наилучшего)
![[IMG_20231212_110823.jpg]]
Дата майнинг - анализ данных с целью извлечения из них ценной информации. 
![[IMG_20231212_111205.jpg]]
Ии - название области (как химия физика)
Мо - раздел ии, но не единственный 
Нейросети - один из видов машинного обучения
Глубокое обучение - архитектура нейросетей

## Модель и процесс машинного обучения
[[Модель машинного обучения]] - функция/алгоритм, который даёт ответ/оценку ситуации по набору входных данных
![[IMG_20231212_111542.jpg]]
![[IMG_20231212_111646.jpg]]
### [[Обучение с учителем]]:
- даём данные и ответ
- добавляем новые данные
Учитель - изначальная [[Выборка|выборка]] с отмеченными нужным образом данными
Виды задач: 
- классификация (x->0, 1...k), k - номера классов
- - регрессия 
- (от количества всех возможных объектов)

### [[Обучение без учителя]]:
Используется при большом количестве некатегоризованных хаотичных данных:
- машина сама находит связи между отдельными данными
- выявляет их закономерности
- подбирает шаблоны
- сортирует объекты в наборе данных
### [[Обучение с подкреплением]]:
Подкрепление - создание связей:
- машина обучается сама, методом проб и ошибок (данные при этом отсутствуют, но есть среда-генератор потока сигналов)
Задача обучения с подкреплением - минимизация количества ошибок (поэтому последовательность успешных и неуспешных решений фиксируется)
### [[Глубокое обучение]]
#### Глубокое обучение - подвид [[Машинное обучение|машинного обучения]]
Состоит из: 
- ввод данных
- извлечение признаков и классификация
- результат
Оно рассматривается как поиск условий, при которых обучаемая модель в большинстве случаев начинает выдавать конкретные ответы. Использует [[Датасет|датасеты]].
### [[Нейросети]] - подвид глубокого обучения
### Ансамбли
[[Ансамбли]] - объединение программ и их общее и исправление ошибок друг друга (например, распознавание предметов)
![[Машинное обучение.canvas]]
1. Выбор источника данных
2. Определение цели/метрики результатов
1+2. ETL - Extract, Transformer, Load - сборка, отчистка, объединение данных
3. EDA - Explorarory Data Analysis
1.₽++_+_ 
2. Данные конвертируются в формат
3. Данные загружаются в хранилище
## Процесс ветвей
1. Необработанные данные из различных источников извлекаются и помещаются в промежуточную область
Форматы разные, поэтому создаётся логическая карта данных. Они описывают свзяь данных с источниками данных и целевыми данными. 
2. Необходимо проверить соответствие исходных данных
3. Отчистка от спама и нежелательных данных
4. Проверка на соответствие данных целевому хранилищу
### Способы извлечения данных
1. Источник уведомляет об изменении данных
2. Частичное извлечение без уведомления (источник может указывать на данные изменения)
3. Извлечение данных полное

#### 2 шаг -  преобразование
На этом этапе необработанные данные
Устранение пропусков:
Интерполяция занчений ( заполнение близкими значения)
не работает с категориальными данными
Дедупликация (исключение лишних данных)
Стандартизация (преобразование разных типов данных к одному)
Проверка (удаление неиспользуемых данных и отметка аномалий)
Повторная сортировка столбцов и строк
Объединение данных из нескольких значений в одно
Дополнение данных из других источников
#### 3 шаг (load)
Обработанные данные из промежуточной области загружаются в целевую базу хранилища
Варианты загрузки


2. Инкрементая загрузка (периодическая запись новых данных по мере необходимости)
3. Полное обновление таблицы (для последних данных)

преимущества ETL:
- экономия времени и автоматизация обработки данных
- упрощение работы со сложными данными
- снижение рисков из человеческого фактора
- улучшение процесса принятия решений и рентабельности инвестиций
Выбор формата обработки данных имеет приоритет
Масштабируемость данных

ELT - современный взгляд на ETL (поменяли буквы местами)
Внедрение ETL: 
- чёткое определение источников данных
- применение правил (агрегация, сортировка...)
- После преобразования данных их необходимо загружать с определённой периодичности
- Перемещение данных из хранилища
- автоматизация процесса ETL с помощью специальных инструментов
### EDA
Данный процесс выполняется единократно для каждой задачи машинного обучения
#### задачи EDA
1. Выбрать простейшую модель машинного обучения, выдающую наиболее точный результат
2. Поиск пропусков и преобразование их в адекватный вид
3. Определиться с наиболее важными параметрами
#### улучшение
1. Нахождение пропусков
2. Оценка параметров распредения данныхй
3. Корреляция между параметрами (для отсеивания ненужных)
4. Корреляция между предсказываемыми значениями
#### подготовка данных
Виды преобразования данных:
1. Нормирование (приведение к одному диапазону, чаще всего [0,1]
$$ x>\frac{x-x_{min}}{x_{max}-{x_{min}}}$$
$$y>\frac{y-y_{min}}{y_{max}-{y_{min}}}$$
2. Категориальные данные (приведение к единичным векторам)
Для этого составляется идентификатор и назначается значение категорий
3. Преобразование циклических переменных - применяют тригонометрические функции
### Разбиение [[Выборка|выборки]]
При машинном обучении данные разбиваются на 
- обучающие
- тестовые
- тренировочные
- валидационные наборы
Hold-out - данные разделяются на обучающие и тестовые выборки (SK_learn). Перед таким способом необходимо перемешивание данных.
В зависимости от обьёма данных выделяются на обучающую и проверочную выборки: 
- 80/20 - для оценки конкурирующих моделей и сравнения по метрикам точности
- 60/40
- 85/15
## [[Нейросети|Нейронные сети]]:
Виды выборок: 
- обучающая
- подтверждающая (необязательная)
- тестовая...
Во всех моделях данные разделяются на обучающие и проверочные.
Для проверки параметров используются [[Кросс-валидация|кросс-валидация]]. Если параметров много - применяется сетка значений.
В сложных моделях, где присутствуют градиентный спуск, 
применяется к-фолд
Для нейросетей обучающую выборку разделяют на две части для дополнительной валидации модели на каждой эпохе обучения нейросети (для избежания переобученив)
#### оптимизация и подбор гиперпараметров
Они настраиваются в процессе обучения на данных
Это характеристики модели, регистрирующиеся до начала обучения
Модель регрессии: $f(x) =X*w$ -w-веса модели, X-матрица модели, где каждая строка содержит признаки одного обьекта выборки.
Модель может обучаться посредством $L=|y-Xw|^2+C|w|^2$, где y - целевая переменная, C - коэффициент точности
Пусть есть несколько моделей разной природы или есть несколько нейросетей.
1. Выборка разделяется на валидационную, тестовую части...?
2. Для каждой модели выбирается гиперпараметры, максимизируеющие метрики модели на валидации
3. Окончательное сравнение проводится по тестовым меркам
Кросс валидация используется, если данныз мало или если существует необходимость избежания зависимости от конкретного валидационного множества.
1. Тестируется и откладывается некоторое тестируемое множество
2. Разделение оставшегося множества на к фолды
	1. Осуществляется проход по циклу
	2. На каждой итерации фиксируется один фолд в качестве валидационного, а остальные используются для обучения
3. В качестве мер оценки модели берётся среднее значение валидационной метрики по фолдам
4. Финальное сравнение моделей с уже подобранными гиперпараметрами проводится на отложенном тестовом множестве

Методы поиска гиперпараметров:
- Поиск некоторых гиперпараметров можно вести по логарифмической шкале, ускоряя его. В данном случае, поиск по сетке выполняется по перекрёстной валидации.
- случайный поиск (если большое количество комбинаций параметров). 
	- можно взять меньше значений каждого гиперпараметра за счёт уменьшения шанса на наилучшую комбинацию.
	- можно уменьшить число фолдов в кросс-валидации, но при этом оценка параметров станет более "шумной"
	- можно перебирать параметры последовательно, а не комбинируя их. Но в этом случае имеется шанс на получение неоптимального решения
	- можно перебирать не все комбинации гиперпараметров, а только случайное подмножество
### Недообучение и переобучение
Недообучение - явление, когда ошибка на выходе достаточно большая
Причины:
- малое количество эпох обучения (ранняя остановка обучения);
- неоптимальные гиперпараметры;
- неправильная функция ошибки;
- **неверная модель**.
Переобучение - явление, когда ошибка заметн о больше на обучающей выборке
Для решения проблемы переобучения была введена проверка на валидационной выборке - данных из обучающей выборки, не участвующие в обучении
### Разброс, смещение и ошибка данных
Центр - идеальное состояние данных
Смещение (bias) - расстояние между математическим ожиданием данных и математическим ожиданием самой модели (математическое ожидание между истинным ответом и выданным алгоритмом). Характеризует способность модели настраиваться на целевую функцию.
Разброс - отклонение от математического ожидания самой модели. Дисперсия ответов алгоритма. Характеризует разнообразие алгоритма
### сложность модели алгоритма
Этот показатель отображает разнообразие семейство алгоритма модели с точки зрения функциональных средств.
Повышение сложности решает недообучение (снижает смещение) и вызывает переобучение (повышает разброс)

Ошибка данных складывается из статического шума и последовательности:
Статический шум - случайность измерения ($±\sqrt{D}$) - в таких рамках относится к нему.

### Регрессия 
Регрессия - задача машинного обучения с учителем в предсказании некоторой непрерывной величины.
Для регрессионных моделей необходимо наличие характеристик объектов и правильное значение целевой переменной
Задача регрессии основывается на существовании взаимосвязи целевой переменной от значения признаков
Регрессиионная модель принимает набор данных и выдаёт предполагаемый значения целевой переменной.
 

### прогнозирование:
- минмакс - требует знание статистики и имеет низкую точность (при разбросе, низком количестве данных)
- линейная регрессия
Особенности выборки:
- плотность формул
- распределение
- мин/макс значения (квантили выборки)
- #### Линейная регрессия
Это модель зависимости переменной от одной или нескольких других переменных с линейной функций зависимости.
Цель - построить линию, которая лучше всего соответствует множеству точек.
Коэффициент корреляции считается на основе функции, чем он ближе к единице - тем теснее связь.
Восстановление функции линейной регрессии позволяет определить y по x, что есть задача прогнозирования
Для построения линии регрессии, необходимо учитывать данные, основывающие модель.
****Фото****
В уравнении б0 - свободный член, а б1 - угловой коэффициент 
*Фото2* между предсказаннвм и истинным значением 
В уравнении б0 - свободный член, а б1 - угловой коэффициент, ссе - сумма квадратов ошибок
***Фото3*** разности между разными значениями y и вычисленными по сравнению (y-) - называют отклонениями

величины прогноза являются 
а отклонения показывают отличие от ожидаемой модели 
В линейной регрессии y - есть сумма её мат. ожидания и случайного отклонения эпсилон. 
#### Определение стандартной ошибки 
Для оценки разброса используется стандартная ошибка - степень отличия реальных значений y от оценочных. 
#### определение неопределенности
Источники: 
- от выборочной прямой регрессии
- от регрессионной прямой генеральной совокупности 
Интервальный график можно построить с учётом обоих источников неопределенности
Стандартная ошибка прогноза - вариативность
*Фото4*, где т - квантиль распределения стьюдента с двумя степенями свободы, дф= н-2
#### предположения в модели регресиии:
- в генеральной совокупности у имеет нормальное распределение
- совокупность точек относительно регресиионой прямой остаётся постоянной
- слагаемые ошибок эпсилон не зависят между собой
- в генеральной совокупности существует линейная зависимости х и у